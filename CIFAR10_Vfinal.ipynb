{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10 Vfinal.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxqF6riGLtziJmh0Gs03yn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauloemp/CIFAR10/blob/main/CIFAR10_Vfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dSODuIvezpe",
        "outputId": "c296bcd4-9ded-4692-c511-6ef4435dc842"
      },
      "source": [
        "#Esse trabalho teve como base dois artigos\n",
        "#[1] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n",
        "#[2] Shuying Liu and Weihong Deng. Very deep convolutional neural network based image classifi- cation using small training sample size. In Pattern Recognition (ACPR), 2015 3rd IAPR Asian Conference on, pages 730–734. IEEE, 2015.\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "\n",
        "class cifar10vgg:\n",
        "    def __init__(self,train=True):\n",
        "        self.num_classes = 10\n",
        "        self.weight_decay = 0.0005\n",
        "        self.x_shape = [32,32,3]\n",
        "\n",
        "        self.model = self.build_model()\n",
        "        if train:\n",
        "            self.model = self.train(self.model)\n",
        "        else:\n",
        "            self.model.load_weights('cifar10vgg.h5')\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        #Construindo a rede da vgg para 10 classes com alto valor de dropout e redução de peso, conforme descrito no artigo.\n",
        "\n",
        "        model = Sequential()\n",
        "        weight_decay = self.weight_decay\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), padding='same', input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(self.num_classes))\n",
        "        model.add(Activation('softmax'))\n",
        "        return model\n",
        "\n",
        "\n",
        "    def normalize(self,X_train,X_test):\n",
        "        #Esta função normaliza as entradas para média zero e variância unitária\n",
        "        # \n",
        "        # Input: training set and test set\n",
        "        # Output: normalized training set and test set according to the trianing set statistics.\n",
        "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
        "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "        X_train = (X_train-mean)/(std+1e-7)\n",
        "        X_test = (X_test-mean)/(std+1e-7)\n",
        "        return X_train, X_test\n",
        "\n",
        "    def normalize_production(self,x):\n",
        "        #this function is used to normalize instances in production according to saved training set statistics\n",
        "        # Input: X - a training set\n",
        "        # Output X - a normalized training set according to normalization constants.\n",
        "\n",
        "        #these values produced during first training and are general for the standard cifar10 training set normalization\n",
        "        mean = 120.707\n",
        "        std = 64.15\n",
        "        return (x-mean)/(std+1e-7)\n",
        "\n",
        "    def predict(self,x,normalize=True,batch_size=50):\n",
        "        if normalize:\n",
        "            x = self.normalize_production(x)\n",
        "        return self.model.predict(x,batch_size)\n",
        "\n",
        "    def train(self,model):\n",
        "\n",
        "        #training parameters\n",
        "        batch_size = 128\n",
        "        maxepoches = 60\n",
        "        learning_rate = 0.1\n",
        "        lr_decay = 1e-6\n",
        "        lr_drop = 20\n",
        "        # The data, shuffled and split between train and test sets:\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train, x_test = self.normalize(x_train, x_test)\n",
        "\n",
        "        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n",
        "        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n",
        "\n",
        "        def lr_scheduler(epoch):\n",
        "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "        reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "        #data augmentation\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False)  # randomly flip images\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "\n",
        "\n",
        "        #optimization details\n",
        "        sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "\n",
        "        # training process in a for loop with learning rate drop every 25 epoches.\n",
        "\n",
        "        historytemp = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                         batch_size=batch_size),\n",
        "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                            epochs=maxepoches,\n",
        "                            validation_data=(x_test, y_test),callbacks=[reduce_lr],verbose=1)\n",
        "        model.save_weights('cifar10vgg.h5')\n",
        "        return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "    model = cifar10vgg()\n",
        "\n",
        "    predicted_x = model.predict(x_test)\n",
        "    residuals = np.argmax(predicted_x,1)!=np.argmax(y_test,1)\n",
        "\n",
        "    loss = sum(residuals)/len(residuals)\n",
        "    score = model.evaluate(x_test, y_test)\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "390/390 [==============================] - 72s 92ms/step - loss: 21.3181 - accuracy: 0.1582 - val_loss: 15.3216 - val_accuracy: 0.1246\n",
            "Epoch 2/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 12.7787 - accuracy: 0.2800 - val_loss: 9.1641 - val_accuracy: 0.1120\n",
            "Epoch 3/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 6.7347 - accuracy: 0.3631 - val_loss: 4.9404 - val_accuracy: 0.2305\n",
            "Epoch 4/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 3.9427 - accuracy: 0.4360 - val_loss: 3.2109 - val_accuracy: 0.3784\n",
            "Epoch 5/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 2.6374 - accuracy: 0.5179 - val_loss: 2.2622 - val_accuracy: 0.5440\n",
            "Epoch 6/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 2.0189 - accuracy: 0.5803 - val_loss: 1.7356 - val_accuracy: 0.6205\n",
            "Epoch 7/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.7023 - accuracy: 0.6261 - val_loss: 1.7783 - val_accuracy: 0.5988\n",
            "Epoch 8/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.5806 - accuracy: 0.6540 - val_loss: 1.3906 - val_accuracy: 0.7155\n",
            "Epoch 9/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.4978 - accuracy: 0.6786 - val_loss: 1.4563 - val_accuracy: 0.7039\n",
            "Epoch 10/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.4703 - accuracy: 0.6964 - val_loss: 1.3420 - val_accuracy: 0.7286\n",
            "Epoch 11/60\n",
            "390/390 [==============================] - 34s 88ms/step - loss: 1.4563 - accuracy: 0.7039 - val_loss: 1.4845 - val_accuracy: 0.7039\n",
            "Epoch 12/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.4414 - accuracy: 0.7150 - val_loss: 1.4555 - val_accuracy: 0.7131\n",
            "Epoch 13/60\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 1.4398 - accuracy: 0.7212 - val_loss: 1.3809 - val_accuracy: 0.7413\n",
            "Epoch 14/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.4508 - accuracy: 0.7229 - val_loss: 1.5353 - val_accuracy: 0.7028\n",
            "Epoch 15/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.4643 - accuracy: 0.7240 - val_loss: 1.3799 - val_accuracy: 0.7546\n",
            "Epoch 16/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.4840 - accuracy: 0.7267 - val_loss: 1.4232 - val_accuracy: 0.7545\n",
            "Epoch 17/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.4685 - accuracy: 0.7347 - val_loss: 1.4931 - val_accuracy: 0.7245\n",
            "Epoch 18/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.4878 - accuracy: 0.7353 - val_loss: 1.4727 - val_accuracy: 0.7492\n",
            "Epoch 19/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.4781 - accuracy: 0.7430 - val_loss: 1.4636 - val_accuracy: 0.7438\n",
            "Epoch 20/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.5046 - accuracy: 0.7369 - val_loss: 1.4725 - val_accuracy: 0.7454\n",
            "Epoch 21/60\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 1.3682 - accuracy: 0.7746 - val_loss: 1.2068 - val_accuracy: 0.8076\n",
            "Epoch 22/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.2222 - accuracy: 0.7953 - val_loss: 1.1238 - val_accuracy: 0.8168\n",
            "Epoch 23/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.1876 - accuracy: 0.7957 - val_loss: 1.1047 - val_accuracy: 0.8144\n",
            "Epoch 24/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.1933 - accuracy: 0.7888 - val_loss: 1.1288 - val_accuracy: 0.8123\n",
            "Epoch 25/60\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 1.1814 - accuracy: 0.7944 - val_loss: 1.1405 - val_accuracy: 0.8107\n",
            "Epoch 26/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.1915 - accuracy: 0.7956 - val_loss: 1.1074 - val_accuracy: 0.8201\n",
            "Epoch 27/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.1865 - accuracy: 0.7971 - val_loss: 1.1022 - val_accuracy: 0.8254\n",
            "Epoch 28/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.1961 - accuracy: 0.7965 - val_loss: 1.1391 - val_accuracy: 0.8188\n",
            "Epoch 29/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.1953 - accuracy: 0.7983 - val_loss: 1.1871 - val_accuracy: 0.8075\n",
            "Epoch 30/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.2105 - accuracy: 0.7949 - val_loss: 1.1575 - val_accuracy: 0.8169\n",
            "Epoch 31/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.1976 - accuracy: 0.8042 - val_loss: 1.1537 - val_accuracy: 0.8164\n",
            "Epoch 32/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.2139 - accuracy: 0.8007 - val_loss: 1.2084 - val_accuracy: 0.7979\n",
            "Epoch 33/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.2152 - accuracy: 0.8015 - val_loss: 1.1587 - val_accuracy: 0.8222\n",
            "Epoch 34/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.2200 - accuracy: 0.8022 - val_loss: 1.1501 - val_accuracy: 0.8247\n",
            "Epoch 35/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.1964 - accuracy: 0.8096 - val_loss: 1.2141 - val_accuracy: 0.8055\n",
            "Epoch 36/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.2104 - accuracy: 0.8055 - val_loss: 1.1600 - val_accuracy: 0.8222\n",
            "Epoch 37/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.2094 - accuracy: 0.8075 - val_loss: 1.3034 - val_accuracy: 0.7840\n",
            "Epoch 38/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.2195 - accuracy: 0.8042 - val_loss: 1.1840 - val_accuracy: 0.8018\n",
            "Epoch 39/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.2198 - accuracy: 0.8049 - val_loss: 1.1618 - val_accuracy: 0.8318\n",
            "Epoch 40/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.2284 - accuracy: 0.8063 - val_loss: 1.2487 - val_accuracy: 0.7955\n",
            "Epoch 41/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 1.1422 - accuracy: 0.8293 - val_loss: 0.9633 - val_accuracy: 0.8738\n",
            "Epoch 42/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 1.0278 - accuracy: 0.8510 - val_loss: 0.9222 - val_accuracy: 0.8723\n",
            "Epoch 43/60\n",
            "390/390 [==============================] - 34s 86ms/step - loss: 0.9882 - accuracy: 0.8501 - val_loss: 0.8990 - val_accuracy: 0.8734\n",
            "Epoch 44/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 0.9624 - accuracy: 0.8494 - val_loss: 0.9204 - val_accuracy: 0.8575\n",
            "Epoch 45/60\n",
            "390/390 [==============================] - 34s 87ms/step - loss: 0.9549 - accuracy: 0.8505 - val_loss: 0.8962 - val_accuracy: 0.8649\n",
            "Epoch 46/60\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9558 - accuracy: 0.8477"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxV2Vklpe1Pb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}